{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cbcf5e-2945-4990-9d6d-e26f8f2e8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/madelinemiller/Desktop/data_literacy/geonews_femicide/source/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "#install libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tueplots\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05ff78a-4dbd-4a9f-b2fc-0aa0a77143d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set source and output paths\n",
    "source_path = '../../data/'\n",
    "csv_output_path = '../../data/processed/'\n",
    "figure_output_path = '../../paper/figures/'\n",
    "\n",
    "#upload raw query data\n",
    "df_32_raw = pd.read_csv(f'{source_path}repository_queries/500000_32_homicide-female_DE.csv') \n",
    "#upload manually tagged articles with json data\n",
    "df_tag = pd.read_csv(f'{source_path}processed/manual-tag_all_parsedson.csv') \n",
    "#upload keyword data\n",
    "df_key = pd.read_csv(f'{source_path}manual_tag/femicide_keywords.csv')\n",
    "#upload top 25 data\n",
    "df_25 = pd.read_csv(f'{source_path}processed/7-14_22-24_26-27_29-32_2017-2023_top25.csv')\n",
    "\n",
    "#filter to only one entry per NUTS\n",
    "df_32_raw['NUTS'] = df_32_raw['NUTS'].fillna('').astype(str)\n",
    "df_32 = df_32_raw.groupby('id').agg({\n",
    "    'NUTS': lambda x: ', '.join(sorted(set(code for code in x if code.startswith('DE')))),\n",
    "    'url': 'first',\n",
    "    'hostname': 'first',\n",
    "    'date': 'first',\n",
    "    'cos_dist': 'first' # these values will all be the same\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431844c-872a-4e22-b251-d9907092c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#headline analysis\n",
    "#how many headlines are duplicates from the same source?\n",
    "\n",
    "#set to all lowercase\n",
    "\n",
    "#remove leading/trailing spaces\n",
    "\n",
    "#count duplicates: same headline, same source --> how many times are they duplicated?\n",
    "\n",
    "#return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11c529f-e1c2-4add-9525-ec489ec75dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows after processing: 5516\n",
      "\n",
      "Keyword Summary:\n",
      "             keyword  total_articles  relevant_count  irrelevant_count\n",
      "38               tat             525             329               188\n",
      "4                ehe             476             295               176\n",
      "29              mord             354             223               123\n",
      "26            gewalt             320             186               129\n",
      "43               tot             309             196               109\n",
      "25           getötet             302             208                92\n",
      "34             opfer             289             166               120\n",
      "40               tod             289             211                75\n",
      "35           partner             221             111               106\n",
      "51        verdächtig             220             166                54\n",
      "52       verdächtige             199             150                49\n",
      "30             morde             192             105                84\n",
      "47            tötung             170             117                53\n",
      "44              tote             148              89                56\n",
      "45         totschlag             114              77                36\n",
      "53      verdächtigen             102              75                27\n",
      "32           mädchen             100              53                44\n",
      "46            tötete              98              73                25\n",
      "7            ehemann              97              73                24\n",
      "49     tötungsdelikt              94              64                30\n",
      "21           femizid              94              43                49\n",
      "15          ermordet              90              48                39\n",
      "22          femizide              68              25                42\n",
      "36         partnerin              66              33                33\n",
      "5            ehefrau              60              48                12\n",
      "27         gewalttat              56              29                26\n",
      "55          weiblich              48              26                22\n",
      "48          tötungen              40              19                21\n",
      "13         ehrenmord              37              12                25\n",
      "56         weibliche              36              15                21\n",
      "50    tötungsdelikte              28              13                15\n",
      "14        ehrenmorde              27               6                21\n",
      "31          mordfall              27              18                 9\n",
      "23        frauenmord              26               9                17\n",
      "3            bluttat              25              18                 4\n",
      "24       frauenmorde              21               6                15\n",
      "16         ermordete              21              15                 6\n",
      "0    beziehungsdrama              19               7                12\n",
      "28       gewalttätig              18               9                 9\n",
      "37      partnerinnen              10               2                 8\n",
      "54       verheiratet              10               6                 4\n",
      "18          femicide              10               1                 9\n",
      "17        ermordeten              10               5                 5\n",
      "41         todesfall               6               6                 0\n",
      "19         feminizid               5               0                 4\n",
      "8          ehemänner               5               4                 1\n",
      "9         ehepartner               4               2                 2\n",
      "33               nan               4               1                 2\n",
      "2            blutige               4               2                 2\n",
      "58        weiblicher               4               1                 3\n",
      "42        todesfälle               3               3                 0\n",
      "20        feminizide               3               0                 3\n",
      "1   beziehungsdramen               2               1                 1\n",
      "11   ehepartnerinnen               2               1                 1\n",
      "10      ehepartnerin               2               1                 1\n",
      "6          ehefrauen               2               1                 1\n",
      "57        weiblichem               2               1                 1\n",
      "39             tated               1               1                 0\n",
      "12       ehetragödie               1               0                 1\n"
     ]
    }
   ],
   "source": [
    "# Keyword Analysis: \n",
    "# For each keyword, get a count of how many relevant and irrelevant articles are associated with it\n",
    "\n",
    "# Ensure the keyword column is lowercase for matching\n",
    "df_key['keyword'] = df_key['keyword'].str.lower()\n",
    "df_tag['found_keywords'] = df_tag['found_keywords'].str.lower()\n",
    "df_tag['not_found_keywords'] = df_tag['not_found_keywords'].str.lower()\n",
    "\n",
    "# Make sure keywords are strings\n",
    "df_tag['found_keywords'] = df_tag['found_keywords'].astype(str)\n",
    "\n",
    "# Filter rows where 'has_keyword_data' equals True (boolean)\n",
    "df_tag_key = df_tag[df_tag['has_keyword_data'] == True].copy()\n",
    "\n",
    "# Split by comma\n",
    "df_tag_key['found_keywords_list'] = df_tag_key['found_keywords'].str.split(',')\n",
    "\n",
    "# Remove extra whitespace and convert to lowercase\n",
    "df_tag_key['found_keywords_list'] = df_tag_key['found_keywords_list'].apply(\n",
    "    lambda x: [kw.strip().lower() for kw in x] if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "# Explode into multiple rows\n",
    "df_tag_key = df_tag_key.explode('found_keywords_list')\n",
    "\n",
    "# Remove empty keywords and NaN values\n",
    "df_tag_key = df_tag_key[df_tag_key['found_keywords_list'].notna()]\n",
    "df_tag_key = df_tag_key[df_tag_key['found_keywords_list'] != '']\n",
    "\n",
    "print(f\"Total rows after processing: {len(df_tag_key)}\")\n",
    "\n",
    "# Create summary: count of relevant and irrelevant articles per keyword\n",
    "# Assuming you have a column that indicates relevance (adjust column name as needed)\n",
    "# Common column names might be: 'relevant', 'is_relevant', 'woman_murdered', etc.\n",
    "\n",
    "keyword_summary = df_tag_key.groupby('found_keywords_list').agg(\n",
    "    total_articles=('id', 'nunique'),\n",
    "    relevant_count=('woman_murdered', lambda x: (x == True).sum()),\n",
    "    irrelevant_count=('woman_murdered', lambda x: (x == False).sum())\n",
    ").reset_index()\n",
    "\n",
    "keyword_summary.columns = ['keyword', 'total_articles', 'relevant_count', 'irrelevant_count']\n",
    "\n",
    "# Sort by total articles descending\n",
    "keyword_summary = keyword_summary.sort_values('total_articles', ascending=False)\n",
    "\n",
    "print(f\"\\nKeyword Summary:\")\n",
    "print(keyword_summary)\n",
    "\n",
    "keyword_summary.to_csv(f'{csv_output_path}keyword_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34977424-ea9a-4a72-8762-32ac232949fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relevance metrics for each keyword\n",
    "keyword_summary['relevance_rate'] = (\n",
    "    keyword_summary['relevant_count'] / keyword_summary['total_articles'] * 100\n",
    ")\n",
    "\n",
    "# Calculate irrelevance rate\n",
    "keyword_summary['irrelevance_rate'] = (\n",
    "    keyword_summary['irrelevant_count'] / keyword_summary['total_articles'] * 100\n",
    ")\n",
    "\n",
    "# Filter keywords with at least 20 articles for more reliable statistics\n",
    "keyword_filtered = keyword_summary[keyword_summary['total_articles'] >= 20].copy()\n",
    "\n",
    "# Sort by relevance rate\n",
    "keyword_filtered = keyword_filtered.sort_values('relevance_rate', ascending=False)\n",
    "\n",
    "print(\"Top 10 Keywords - Highest Relevance Rate:\")\n",
    "print(keyword_filtered[['keyword', 'total_articles', 'relevance_rate', 'relevant_count', 'irrelevant_count']].head(10))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Top 10 Keywords - Lowest Relevance Rate (Predictors of Irrelevance):\")\n",
    "print(keyword_filtered[['keyword', 'total_articles', 'relevance_rate', 'relevant_count', 'irrelevant_count']].tail(10))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Top 20 keywords by relevance rate (min 20 articles)\n",
    "ax1 = axes[0, 0]\n",
    "top_relevant = keyword_filtered.head(20)\n",
    "ax1.barh(range(len(top_relevant)), top_relevant['relevance_rate'], color='steelblue')\n",
    "ax1.set_yticks(range(len(top_relevant)))\n",
    "ax1.set_yticklabels(top_relevant['keyword'])\n",
    "ax1.set_xlabel('Relevance Rate (%)')\n",
    "ax1.set_title('Top 20 Keywords by Relevance Rate\\n(min 20 articles)', fontsize=12, fontweight='bold')\n",
    "ax1.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax1.legend()\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# 2. Scatter plot: Total articles vs Relevance rate\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(keyword_filtered['total_articles'], \n",
    "                     keyword_filtered['relevance_rate'],\n",
    "                     s=keyword_filtered['total_articles']*2,\n",
    "                     alpha=0.6,\n",
    "                     c=keyword_filtered['relevance_rate'],\n",
    "                     cmap='RdYlGn')\n",
    "ax2.set_xlabel('Total Articles')\n",
    "ax2.set_ylabel('Relevance Rate (%)')\n",
    "ax2.set_title('Keyword Frequency vs Relevance Rate', fontsize=12, fontweight='bold')\n",
    "ax2.axhline(y=50, color='red', linestyle='--', alpha=0.5)\n",
    "plt.colorbar(scatter, ax=ax2, label='Relevance %')\n",
    "\n",
    "# Annotate key outliers\n",
    "for idx, row in keyword_filtered.iterrows():\n",
    "    if row['total_articles'] > 200 or row['relevance_rate'] > 75 or row['relevance_rate'] < 40:\n",
    "        ax2.annotate(row['keyword'], \n",
    "                    (row['total_articles'], row['relevance_rate']),\n",
    "                    fontsize=8, alpha=0.7)\n",
    "\n",
    "# 3. Stacked bar chart - Relevant vs Irrelevant (top 20 by volume)\n",
    "ax3 = axes[1, 0]\n",
    "top_volume = keyword_filtered.nlargest(20, 'total_articles')\n",
    "x_pos = range(len(top_volume))\n",
    "ax3.bar(x_pos, top_volume['relevant_count'], label='Relevant', color='green', alpha=0.7)\n",
    "ax3.bar(x_pos, top_volume['irrelevant_count'], \n",
    "       bottom=top_volume['relevant_count'], label='Irrelevant', color='red', alpha=0.7)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(top_volume['keyword'], rotation=45, ha='right')\n",
    "ax3.set_ylabel('Number of Articles')\n",
    "ax3.set_title('Top 20 Keywords by Volume\\n(Relevant vs Irrelevant)', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Distribution of relevance rates\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(keyword_filtered['relevance_rate'], bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(x=keyword_filtered['relevance_rate'].mean(), \n",
    "           color='red', linestyle='--', linewidth=2, label=f'Mean: {keyword_filtered[\"relevance_rate\"].mean():.1f}%')\n",
    "ax4.axvline(x=keyword_filtered['relevance_rate'].median(), \n",
    "           color='orange', linestyle='--', linewidth=2, label=f'Median: {keyword_filtered[\"relevance_rate\"].median():.1f}%')\n",
    "ax4.set_xlabel('Relevance Rate (%)')\n",
    "ax4.set_ylabel('Number of Keywords')\n",
    "ax4.set_title('Distribution of Keyword Relevance Rates', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SUMMARY (Keywords with 20+ articles)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean Relevance Rate: {keyword_filtered['relevance_rate'].mean():.2f}%\")\n",
    "print(f\"Median Relevance Rate: {keyword_filtered['relevance_rate'].median():.2f}%\")\n",
    "print(f\"Std Dev: {keyword_filtered['relevance_rate'].std():.2f}%\")\n",
    "print(f\"\\nKeywords above 70% relevance (strong predictors): {(keyword_filtered['relevance_rate'] > 70).sum()}\")\n",
    "print(f\"Keywords below 50% relevance (weak/negative predictors): {(keyword_filtered['relevance_rate'] < 50).sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geonews_femicide)",
   "language": "python",
   "name": "geonews_femicide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
